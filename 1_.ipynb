{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### quantization\n",
    "- convertion from higher memory format to a lower memory format"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- consider al llm model with 70 billion parameters.each may be of type FP32(Full precision)-floating point | 32 bits \n",
    "- converting it to int 8 will help low spec pc to operate it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quantization vs Inference in Large Language Models (LLMs)\n",
    "\n",
    "### **1. Quantization**\n",
    "Quantization is a technique used to reduce the precision of the numerical representation of a model's parameters. Its goal is to decrease computational requirements, memory usage, and power consumption while maintaining acceptable accuracy.\n",
    "\n",
    "- **Key Points**:\n",
    "  - Converts floating-point (e.g., `float32`) to lower precision (e.g., `int8`).\n",
    "  - Reduces memory footprint and speeds up computation.\n",
    "  - Types:\n",
    "    - **Post-training quantization**: Applied after training.\n",
    "    - **Quantization-aware training**: Incorporated during training to minimize accuracy loss.\n",
    "\n",
    "---\n",
    "\n",
    "### **2. Inference**\n",
    "Inference is the process of using a trained model to generate predictions or outputs based on input data.\n",
    "\n",
    "- **Key Points**:\n",
    "  - Involves deploying the model for real-world use cases like text generation or classification.\n",
    "  - Optimized for low-latency and efficient performance.\n",
    "  - Techniques include batching, pruning, and hardware-specific optimizations.\n",
    "\n",
    "---\n",
    "\n",
    "### **Comparison Table**\n",
    "\n",
    "| **Aspect**           | **Quantization**                      | **Inference**                       |\n",
    "|-----------------------|---------------------------------------|--------------------------------------|\n",
    "| **Focus**            | Model optimization                   | Generating predictions              |\n",
    "| **Stage**            | Pre-inference optimization            | Deployment and execution phase      |\n",
    "| **Goal**             | Improve efficiency                   | Deliver accurate predictions        |\n",
    "| **Challenges**       | Accuracy trade-offs                  | Real-time response, scaling         |\n",
    "\n",
    "### **Relationship**\n",
    "Quantization is often used as an optimization technique to improve the efficiency of inference, especially for deploying LLMs in resource-constrained environments or scenarios requiring low latency.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calibration in Machine Learning\n",
    "\n",
    "### **What is Calibration?**\n",
    "Calibration refers to the process of adjusting a system to ensure its outputs (predictions or measurements) accurately reflect the intended values. In machine learning, it ensures that the predicted probabilities correspond to the true likelihood of an event.\n",
    "\n",
    "---\n",
    "\n",
    "### **Why is Calibration Needed?**\n",
    "- Many machine learning models output probabilities that may not correspond to real-world outcomes.\n",
    "- Example:\n",
    "  - A model predicts a 70% chance of an event but is only correct 50% of the time.\n",
    "  - This discrepancy can mislead decision-making processes.\n",
    "- Calibration aligns predicted probabilities with actual outcomes.\n",
    "\n",
    "---\n",
    "\n",
    "### **Techniques for Calibration**\n",
    "1. **Platt Scaling**:\n",
    "   - Fits a logistic regression model to map predicted scores to probabilities.\n",
    "   - Suitable for large datasets.\n",
    "\n",
    "2. **Isotonic Regression**:\n",
    "   - Fits a piecewise constant, non-decreasing function to map scores to probabilities.\n",
    "   - Effective for small datasets but prone to overfitting with sparse data.\n",
    "\n",
    "3. **Temperature Scaling**:\n",
    "   - Adjusts logits by multiplying them with a scaling factor (temperature) to improve softmax probabilities.\n",
    "   - Commonly used with deep learning models.\n",
    "\n",
    "4. **Histogram Binning**:\n",
    "   - Groups predictions into bins and adjusts probabilities based on observed frequencies in each bin.\n",
    "   - Simple and interpretable.\n",
    "\n",
    "---\n",
    "\n",
    "### **Calibration Metrics**\n",
    "- **Expected Calibration Error (ECE)**:\n",
    "  Measures the difference between predicted probabilities and observed frequencies across bins.\n",
    "\n",
    "- **Brier Score**:\n",
    "  Combines calibration and sharpness to assess the accuracy of probabilistic predictions.\n",
    "\n",
    "---\n",
    "\n",
    "### **Calibration in Quantization**\n",
    "When quantizing models, calibration ensures that reduced precision (e.g., `int8`) maintains prediction quality:\n",
    "- **Static Quantization**: Calibrates scales and offsets using a representative dataset.\n",
    "- **Dynamic Quantization**: Adjusts scales at runtime based on input.\n",
    "\n",
    "---\n",
    "\n",
    "### **Importance of Calibration**\n",
    "Calibration is critical in applications where confidence in predictions is essential, such as:\n",
    "- Medical diagnostics\n",
    "- Risk assessment\n",
    "- Fraud detection\n",
    "\n",
    "By aligning predicted probabilities with real-world outcomes, calibration improves trust and reliability in model predictions.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "- symmetric quantization\n",
    "   - batch normalization\n",
    "- asymmetric quantization\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
